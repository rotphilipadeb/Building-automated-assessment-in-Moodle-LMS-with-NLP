Moodle is a global open-source Learning Management System (LMS) that is widely used to coordinate classroom activities. While effective for assignments and quizzes, it lacks an automatic grading system for essays, open-ended responses, and discussion forums, which pose additional workloads and restrict deeper evaluation of higher-order thinking skills. To address this gap, we developed an automated assessment plugin for Moodle using Natural Language Processing (NLP). This research identifies the limitations of current assessment systems, designs/builds an automated assessment system/plugin for written responses, evaluates the effectiveness of the NLP model, and assesses how this integration supports sustainable teaching. IT-IDL and BERT models were trained with the ASAP 2.0 dataset from Kaggle, and validated using real student essays and teacher rubrics. The models were integrated into Moodle through a plugin that communicates with an external NLP server via REST API/HTTP. Evaluation metrics such as cosine similarity, part-of-speech (POS) tagging, content evaluation, Pearson Correlation Coefficient, and Cohenâ€™s Kappa were used to compare NLP automated grading with teachers' grading. The results showed a cosine similarity of 0.92, Pearson Correlation of 0.67, Cohen's Kappa of 0.75, and content evaluation of 68%, implying that the NLP model performs strongly in semantic alignment and agreement with human graders, showing potential for automated grading, though content evaluation requires improvement.
